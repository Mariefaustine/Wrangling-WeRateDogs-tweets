WRANGLING AND ANALYZING DATA
This project involved the gathering, assessing, cleaning and storage of archives sourced from the WeRateDogs twitter account. All steps taken were documented in markdown cells in the jupyter notebook.
GATHERING
All needed libraries were imported this included pandas, numpy, json, requests, regex, matplotlib and seaborn. The twitter-archive-enhanced file was read in with the .read_csv function, the image-predictions file was imported and saved with the requests library and the json.txt file was obtained from the classroom and then read in line by line using json.loads(). The json.txt file was not acquired from twitter with tweepy due to issues.
ASSESSING
Visual and Programmatic assessments were both carried out. Visual assessment can be seen in the first three rows. Some visual assessments identified, are the headers that were not descriptive enough, columns with little or no value such as 'in_reply_to_user_id' and in_reply_to_status_id', the date and time, combined in the timestamp column, amongst others. Programmatic assessment which involved the use of functions such as (.shape), (.info) and (.sample) revealed issues such as wrong data types and that some tweets were sourced from vine instead of twitter. The comprehensive list of issues from assessed data was compiled into two sections; Tidiness issues and Quality issues.
CLEANING
The tidiness issues were handled before the quality issues. The first tidiness issue involved, dropping the columns that were common between datasets before concatenating them into ‘master_df’. However, a copy (master_dataset) had to be made in case the original was needed in the future. Afterwards, dog columns were melted into one column and all the duplicates were dropped to return the dataframe to its original shape. Column headers were then made more descriptive using (.rename) function and then unnecessary columns were dropped using (.drop). Next, the data type of the timestamp column was changed to date time for efficiency and then the time was extracted from the column.
The accurate names where available were imputed into the name column one by one using the loc function. After which, tweets sourced from vine were dropped by index. Next, the <a href> tag which was not very pleasing was sliced off. The removal of the tag did not have any effect on the URL (This was confirmed outside of the environment by searching the URL in a web browser). Then the tweet links at the end of text in the text column were split from the text and put into a new column (tweet_urls) for ease of access. Predictions 1, 2 and 3 (p1, p2, p3) columns which were having a mix of both higher and lower cases were converted to all lower cases with the (.str.lower) function and Lastly, different columns were created to remedy the problem of invalid numerators. However the original columns which contained the denominator and numerator separately were left in case of use in the future.
STORAGE
The data has been gathered, assessed for two (2) tidiness and eight (8) quality issues, and all those issues had been cleaned so, to store the data, I used the code {master_dataset.to_csv('twitter_archive_master.csv', index=False)} to save the file as twitter_archive_master as recommended.